---
layout: posts
title: "[논문스터디]YOLO(You Only Look Once)"
categories: 논문
tag: [ML, Study]
toc: true
---

# Abstract

- YOLO 연구진은 객체 검출(object detection)에 새로운 접근방식을 적용했습니다.
- 기존의 multi-task 문제를 **하나의 회귀(regression) 문제**로 재정의했습니다.
- YOLO는 <u>이미지 전체에 대해서 하나의 신경망(a single neural network)이 한 번의 계산만으로 **bounding box**와 클래스 확률(class probability)을 예측</u>합니다.
  -> **bounding box**란 객체의 위치를 알려주기 위해 객체의 둘레를 감싼 직사각형 박스를 말합니다. 클래스 확률이란 bounding box로 둘러싸인 객체가 어떤 클래스에 해당하는지에 관한 확률을 의미합니다.
- 객체 검출 파이프라인이 하나의 신경망으로 구성되어 있으므로 **end-to-end 형식**입니다.
- YOLO의 통합된 모델을 굉장히 빠릅니다. (1초에 45 프레임 처리, Fast YOLO는 1초에 155 프레임 처리)



# 1. Introduction

사람은 이미지를 보면 어디에 무엇이 있는지를 *<u>한 번에 파악</u>*할 수 있습니다. 사람의 시각 체계와 같이 빠르고 정확한 객체 검출 모델을 만들 수 있다면 자율주행차 기술도 급격히 발전할 것입니다. 

기존의 검출(detection) 모델은 분류기(classfier)를 재정의하여 검출기(detector)로 사용하고 있습니다. **분류(classification)**란 하나의 이미지를 보고 그것이 개인지 고양이인지 판단하는 것을 뜻합니다. 하지만 **객체 검출(object detection)**은 하나의 이미지 내에서 개는 어디에 위치해 있고, 고양이는 어디에 위치해 있는지 판단하는 것입니다. 따라서 객체 검출은 **분류뿐만 아니라 위치 정보도 판단**해야 합니다. 기존의 객체 검출 모델로는 대표적으로 **DPM과 R-CNN**이 있습니다.

**Deformable parts models(DPM)**은 이미지 전체를 거쳐 **슬라이딩 윈도(sliding window) 방식으로 객체 검출을 하는 모델**입니다. R-CNN은 이미지 안에서 bounding box를 생성하기 위해 region proposal이라는 방법을 사용합니다. 그렇게 제안된 bounding box에 classifier를 적용하여 분류(classification)합니다. 분류(classification)한 뒤 bounding box를 조정하고, 중복된 검출을 제거하고, 객체에 따라 box의 점수를 재산정하기 위해 후처리(post-processing)를 합니다. 이런 복잡함 때문에 R-CNN은 느립니다. <u>각 절차를 독립적으로 훈련시켜야 하므로 최적화(optimization)하기에도 힘듭니다.</u>

그리하여 YOLO 연구진은 객체 검출을 **하나의 회귀 문제(single regression problem)**로 보고 절차를 개선했습니다. <u>이미지의 픽셀로부터 bounding box의 위치(coordinates), 클래스 확률(class probabilities)을 구하기까지의 일련을 절차를 하나의 회귀 문제로 재정의</u>한 것입니다. 이러한 시스템을 통해 YOLO(you only look once)는 이미지 내에 어떤 물체가 있고 그 물체가 어디에 있는지를 하나의 파이프라인으로 빠르게 구해줍니다. 이미지를 한 번만 보면 객체를 검출할 수 있다 하여 이름이 YOLO(you only look once)입니다.

![img](https://blog.kakaocdn.net/dn/lqVvp/btqKv79n5Pr/lJJ0EoK0sb8kVrLShiMo7k/img.png)

YOLO는 단순합니다. 우선, Figure 1을 봅시다. **하나의 컨볼루션 네트워크(convolutional network)**가 <u>여러 bounding box와 그 bounding box의 클래스 확률을 동시에 계산</u>해 줍니다. YOLO는 이미지 전체를 학습하여 곧바로 **검출 성능(detection performance)을 최적화**합니다. YOLO의 이런 통합된 모델은 기존의 객체 검출 모델에 비해 여러 가지 장점이 있습니다.

첫째, <u>YOLO는 굉장히 빠릅니다</u>. 왜냐하면 YOLO는 기존의 복잡한 객체 검출 프로세스를 하나의 회귀 문제로 바꾸었기 때문입니다. 그리하여 <u>기존의 객체 검출 모델처럼 복잡한 파이프라인이 필요 없습니다</u>. 단순히 테스트 단계에서 새로운 이미지를 YOLO 신경망에 넣어주기만 하면 쉽게 객체 검출을 할 수 있습니다. YOLO의 기본 네트워크(base network)는 Titan X GPU에서 배치 처리(batch processing) 없이 1초에 45 프레임을 처리합니다. 빠른 버전의 YOLO(Fast YOLO)는 1초에 150 프레임을 처리합니다. 이는 동영상을 실시간으로 처리할 수 있다는 의미입니다. (25밀리 초 이하의 지연시간으로 처리 가능) 더욱이 YOLO는 다른 실시간 객체 검출 모델보다 2배 이상의 mAP(mean average precision의 약자로 정확성을 뜻함)를 갖습니다. 데모 버전의 YOLO가 웹캠에 찍힌 영상을 실시간으로 처리하는 것을 보려면 [이곳](https://pjreddie.com/darknet/yolo/)을 참고해주시기 바랍니다.

![img](https://blog.kakaocdn.net/dn/cL9cKl/btqKy788QMR/5ftoDDCloELFQGonSHkY31/img.gif)

둘째, YOLO는 <u>예측을 할 때 이미지 전체를 봅니다</u>. 슬라이딩 윈도(sliding window)나 region proposal 방식과 달리, <u>YOLO는 훈련과 테스트 단계에서 이미지 전체를 봅니다</u>. 그리하여 클래스의 모양에 대한 정보뿐만 아니라 <u>주변 정보까지 학습하여 처리</u>합니다. 반면, YOLO 이전의 객체 검출 모델 중 가장 성능이 좋은 모델인 Fast R-CNN는 주변 정보까지는 처리하지 못합니다. 그래서 아무 물체가 없는 배경(background)에 반점이나 노이즈가 있으면 그것을 물체로 인식합니다. 이를 **background error**라고 합니다. YOLO는 이미지 전체를 처리하기 때문에 background error가 Fast R-CNN에 비해 훨씬 적습니다. (대략 1/2 가량)

셋째, YOLO는 물체의 일반적인 부분을 학습합니다. 일반적인 부분을 학습하기 때문에 자연 이미지를 학습하여 그림 이미지로 테스트할 때, YOLO의 성능은 **DPM**이나 **R-CNN**보다 월등히 뛰어납니다. 따라서 다른 모델에 비해 YOLO는 훈련 단계에서 보지 못한 새로운 이미지에 대해 더 강건(robust)합니다. 즉, 검출 정확도가 더 높다는 뜻입니다.

하지만, YOLO는 최신(SOTA, state-of-the-art) 객체 검출 모델에 비해 <u>정확도가 다소 떨어진다는 단점</u>이 있습니다. 빠르게 객체를 검출할 수 있다는 장점은 있지만 정확성이 다소 떨어집니다. 특히 <u>작은 물체에 대한 검출 정확도가 떨어집니다</u>. 속도와 정확성은 **trade-off 관계**입니다. YOLO의 모든 코드는 오픈 소스이며, 사전 훈련된(pretrained) 모델도 다운받아 사용할 수 있습니다.

[요약]

- YOLO는 **단일 신경망 구조**이기 때문에 구성이 단순하며, 빠르다.
- YOLO는 주변 정보까지 학습하며 이미지 전체를 처리하기 때문에 <u>**background error**가 작다</u>.
- YOLO는 훈련 단계에서 보지 못한 <u>새로운 이미지에 대해서도 검출 정확도가 높다</u>.
- 단, YOLO는 SOTA 객체 검출 모델에 비해 <u>정확도(mAP)가 다소 떨어진다</u>.

# 2. Unified Detection

YOLO는 객체 검출의 개별 요소를 **단일 신경망(single neural network)**으로 통합한 모델입니다. YOLO는 각각의 bounding box를 예측하기 위해 이미지 전체의 특징을 활용합니다. 이러한 YOLO의 디자인 덕분에 높은 정확성을 유지하면서 **end-to-end 학습**과 **실시간 객체 검출**이 가능합니다. 

YOLO는 입력 이미지(input images)를 S x S 그리드(S x S grid)로 나눕니다. 만약 어떤 객체의 중심이 특정 그리드 셀(grid cell) 안에 위치한다면, 그 그리드 셀이 해당 객체를 검출해야 합니다. (If the center of an object falls into a grid cell, that grid cell is responsible for detecting that object.)

각각의 **그리드 셀(grid cell)**은 B개의 bounding box와 그 bounding box에 대한 **confidence score**를 예측합니다. confidence score는 bounding box가 객체를 포함한다는 것을 얼마나 믿을만한지, 그리고 예측한 bounding box가 얼마나 정확한지를 나타냅니다. confidence score는 다음과 같이 정의합니다.

![img](https://blog.kakaocdn.net/dn/b7S5mF/btqKAA3AndN/CrPQJ4uUVDk1fptSufZ4B1/img.png)

여기서 **IOU**는 **intersection over union**의 약자로 <u>객체의 실제 bounding box와 예측 bounding box의 합집합 면적 대비 교집합 면적의 비율을 뜻합니다</u>. 즉, IOU = (실제 bounding box와 예측 bounding box의 교집합) / (실제 bounding box와 예측 bounding box의 합집합)입니다.

만약 그리드 셀에 아무 객체가 없다면 Pr(Obejct)=0입니다. 그러므로 confidence score도 0입니다. 그리드 셀에 어떤 객체가 확실히 있다고 예측했을 때, 즉 Pr(Object)=1일 때가 가장 이상적입니다. 따라서 confidence score가 IOU와 같다면 가장 이상적인 score입니다.

각각의 bounding box는 5개의 예측치로 구성되어 있습니다. x, y, w, h, confidence가 그것입니다. (x, y) 좌표 쌍은 bouning box 중심의 그리드 셀(grid cell) 내 상대 위치를 뜻합니다. <u>절대 위치가 아니라 그리드 셀 내의 상대 위치이므로 0~1 사이의 값을 갖습니다</u>. 만일 bounding box의 중심인 (x, y)가 정확히 그리드 셀 중앙에 위치한다면 (x, y)=(0.5, 0.5)입니다. (w, h) 쌍은 <u>bounding box의 상대 너비와 상대 높이를 뜻합니다</u>. 이때 (w, h)는 이미지 전체의 너비와 높이를 1이라고 했을 때 bounding box의 너비와 높이가 몇인지를 상대적인 값으로 나타냅니다. 그러므로 (w, h)도 역시 0~1 사이의 값을 갖습니다. 마지막으로 confidence는 앞서 다룬 confience score와 동일합니다.

그리고 각각의 그리드 셀은 **conditional class probabilities(C)**를 예측합니다. conditional class probabilities는 다음과 같이 계산할 수 있습니다.

![img](https://blog.kakaocdn.net/dn/bnW22H/btqKuFlpsH7/cGHmZZsP0DJqQkR45GHZ01/img.png)

이는 그리드 셀 안에 객체가 있다는 조건 하에 <u>그 객체가 어떤 클래스(class)인지에 대한 조건부 확률</u>입니다. 그리드 셀에 몇 개의 bounding box가 있는지와는 무관하게 하나의 그리드 셀에는 오직 하나의 클래스(class)에 대한 확률 값만을 구합니다. 하나의 그리드 셀은 B개의 bounding box를 예측한다고 했습니다. B의 개수와는 무관하게 하나의 그리드 셀에서는 클래스 하나만 예측하는 것입니다.

테스트 단계에서는 conditional class probability(C)와 개별 boudning box의 confidence score를 곱해주는데, 이를 각 bounding box에 대한 **class-specific confidence score**라고 부릅니다. class-specific confidence score는 다음과 같이 계산할 수 있습니다. 위에서 구한 conditional class probability와 confidence score를 곱한 값입니다.

![img](https://blog.kakaocdn.net/dn/EWU74/btqKC95t54c/VH2dRhLcG2DX0TOg9TRFTK/img.png)

<u>이 score는 **bounding box**에 특정 클래스(class) 객체가 나타날 확률**(=Pr(Class_i))**과 예측된 bounding box가 그 클래스(class) 객체에 얼마나 잘 들어맞는지(fits the object)(=IOU_pred^truth)를 나타냅니다</u>. (These scores encode both the probability of that class appearing in the box and how well the predicted box fits the object.)

![img](https://blog.kakaocdn.net/dn/pAZ1u/btqKwRZyvs0/rh9WqALOSVg9md4V9Dbmmk/img.png)

YOLO 연구진은 [파스칼 VOC](https://gluon-cv.mxnet.io/build/examples_datasets/pascal_voc.html)라는 이미지 인식 국제대회 데이터 셋을 이용해 실험했습니다. S=7, B=2로 세팅했고 파스칼 VOC는 총 20개의 라벨링 된 클래스가 있으므로 C=20입니다. S=7 이면 인풋 이미지는 7 x 7 그리드로 나뉩니다. B=2이라는 것은 하나의 그리드 셀에서 2개의 bounding box를 예측하겠다는 뜻입니다. 이렇게 했을 때 S x S x (B*5 + C) 텐서를 생성합니다. 따라서 최종 예측 텐서의 dimension은 (7 x 7 x 30)입니다.

[요약]

**2. Unified Detection 요약:**
 \- YOLO는 객체 검출의 개별 요소를 단일 신경망(single neural network)으로 통합한 모델이다.
 \- 입력 이미지(input images)를 S x S 그리드(S x S grid)로 나눈다.
 \- 각각의 그리드 셀(grid cell)은 B개의 bounding box와 그 bounding box에 대한 confidence score를 예측한다.
 \- class-specific confidence score는 bounding box에 특정 클래스(class) 객체가 나타날 확률과 예측된 bounding box가 그 클래스(class) 객체에 얼마나 잘 들어맞는지를 나타낸다.
 \- 최종 예측 텐서의 dimension은 <u>(7 x 7 x 30)</u>이다.

# 2. 1. Network Design

앞서 말씀드렸듯이 이 YOLO 모델을 <u>하나의 CNN(Convolutional Neural Network) 구조로 디자인</u>되었습니다. YOLO 연구진은 파스칼 VOC 데이터셋에 대해 모델링을 수행했습니다. 이 CNN의 앞단은 컨볼루션 계층(convolutional layer)이고, 이어서 **전결합 계층(fully-connected layer)**으로 구성되어 있습니다. 컨볼루션 계층(convolutional layer)은 이미지로부터 특징을 추출하고, 전결합 계층(fully connected layer)은 클래스 확률과 bounding box의 좌표(coordinates)를 예측합니다.

YOLO의 신경망 구조는 이미지 분류(image classification)에 사용되는 **GoogLeNet**에서 따왔습니다. YOLO는 총 <u>24개의 컨볼루션 계층(convolutional layers)과 2개의 전결합 계층(fully connected layers)으로 구성</u>되어 있습니다. GoogLeNet의 인셉션 구조 대신 YOLO는 1 x 1 **축소 계층(reduction layer)**과 <u>3 x 3 컨볼루션 계층의 결합을 사용</u>했습니다. 1 x 1 축소 계층(reduction layer)과 3 x 3 컨볼루션 계층의 결합이 인셉션 구조를 대신한다고 합니다. YOLO 모델의 전체 구조는 다음과 같습니다. 이 모델의 최종 아웃풋은 <u>7 x 7 x 30의 **예측 텐서(prediction tensors)**</u>입니다.

![img](https://blog.kakaocdn.net/dn/bcD1Ts/btqKBPsHQGp/Ku8dlxjrsyWFbcjv6XMnqk/img.png)

좀 더 빠른 객체 인식 속도를 위해 YOLO보다 더 적은 컨볼루션 계층(24개 대신 9개)과 필터를 사용하는 Fast YOLO라는 것도 있습니다. 크기만 다를 뿐이고 훈련 및 테스트 시 사용하는 나머지 파라미터는 YOLO와 모두 동일합니다.

![img](https://curt-park.github.io/images/yolo/DeepSystems-NetworkArchitecture.JPG)



![img](https://curt-park.github.io/images/yolo/NetArch0.JPG)

7X7은 49개의 Grid Cell을 의미한다. 그리고 각각의 Grid cell은 B개의 bounding Box를 가지고 있는데(여기선 B=2), 앞 5개의 값은 해당 Grid cell의 첫 번째 bounding box에 대한 값이 채워져있다.

![img](https://curt-park.github.io/images/yolo/NetArch1.JPG)

6~10번째 값은 두 번째 bounding box에 대한 내용이다.

![img](https://curt-park.github.io/images/yolo/NetArch2.JPG)

나머지 20개의 값은 20개의 class에 대한 conditional class probability에 해당한다.

![img](https://curt-park.github.io/images/yolo/NetArch3.JPG)

첫 번째 bounding box의 confidence score와 각 conditional class probability를 곱하면 첫 번째 bounding box의 class specific confidence score가 나온다.
마찬가지로, 두 번째 bounding box의 confidence score와 각 conditional class probability를 곱하면 두 번째 bounding box의 class specific confidence score가 나온다.

![img](https://curt-park.github.io/images/yolo/NetArch4.JPG)

이 계산을 각 bounding box에 대해 하게되면 총 98개의 class specific confidence score를 얻을 수 있다.

이 98개의 class specific confidence score에 대해 각 20개의 클래스를 기준으로 [non-maximum suppression](https://goo.gl/byNZTn)을 하여, Object에 대한 Class 및 bounding box Location를 결정한다.
논문에는 따로 기술되지 않았지만 Error를 줄이기 위해 class specific confidence score에 대한 Threshold를 설정하지 않았을까 싶다.

# 2. 2. Training

우선, 1,000개의 클래스를 갖는 ImageNet 데이터 셋으로 YOLO의 컨볼루션 계층을 **사전훈련(pretrain)**시켰습니다. 사전훈련을 위해서 24개의 컨볼루션 계층 중 첫 20개의 컨볼루션 계층만 사용했고, 이어서 <u>전결합 계층을 연결</u>했습니다. 이 모델을 약 1주간 훈련시켰습니다. 이렇게 사전 훈련된 모델은 ImageNet 2012 검증 데이터 셋에서 88%의 정확도를 기록했습니다. YOLO 연구진은 이 모든 훈련(training)과 추론(inference)을 위해 Darknet 프레임워크를 사용했습니다. 

**Darknet 프레임워크**는 YOLO를 개발한 Joseph Redmon이 <u>독자적으로 개발한 신경망 프레임워크</u>입니다. 신경망들을 학습하거나 실행할 수 있는 프레임워크로 YOLO도 Darknet에서 학습된 모델 중 하나입니다. 

ImageNet은 <u>분류(classification)를 위한 데이터 셋</u>입니다. 따라서 사전 훈련된 분류 모델을 **객체 검출(object detection) 모델**로 바꾸어야 합니다. 연구진은 사전 훈련된 20개의 컨볼루션 계층 뒤에 <u>4개의 컨볼루션 계층 및 2개의 전결합 계층을 추가하여 성능을 향상</u>시켰습니다. 4개의 컨볼루션 계층 및 2개의 전결합 계층을 추가할 때, 이 계층의 가중치(weights)는 임의로 초기화했습니다. 또한, <u>객체 검출을 위해서는 이미지 정보의 해상도가 높아야 합니다. 따라서 입력 이미지의 해상도를 224 x 224에서 448 x 448로 증가시켰습니다</u>.

이 신경망의 최종 아웃풋(예측값)은 클래스 확률(class probabilities)과 bounding box 위치정보(coordinates)입니다. bounding box의 위치정보에는 bounding box의 너비(width)와 높이(height)와 bounding box의 중심 좌표(x, y)가 있습니다. YOLO 연구진은 너비, 높이, 중심 좌표값(w, h, x, y)을 모두 0~1 사이의 값으로 정규화(normalize)했습니다. 

YOLO 신경망의 마지막 계층에는 **선형 활성화 함수(linear activation function)**를 적용했고, 나머지 모든 계층에는 **leaky ReLU**를 적용했습니다. ReLU는 0 이하의 값은 모두 0인데 비해, leaky ReLU는 0 이하의 값도 작은 음수 값을 갖습니다. 

![img](https://blog.kakaocdn.net/dn/uxlmh/btqKwseIbvE/bXiLqmv6GswUskK8TAOUjK/img.png)

YOLO의 loss는 **SSE(sum-squared error)**를 기반으로 합니다. 따라서 최종 아웃풋의 SSE(sum-squared error)를 최적화(optimize) 해야 합니다. SSE를 사용한 이유는 SSE가 최적화하기 쉽기 때문입니다. <u>하지만 SSE를 최적화하는 것이 YOLO의 최종 목적인 mAP(평균 정확도)를 높이는 것과 완벽하게 일치하지는 않습니다.</u> (however it does not perfectly align with our goal of maximizing average precision.) YOLO의 loss에는 bounding box의 위치를 얼마나 잘 예측했는지에 대한 loss인 localization loss와 클래스를 얼마나 잘 예측했는지에 대한 loss인 classification loss가 있습니다. localization loss와 classification loss의 가중치를 동일하게 두고 학습시키는 것은 좋은 방법이 아닙니다. 하지만 <u>SSE를 최적화하는 방식은 이 두 loss의 가중치를 동일하게 취급합니다</u>.[SSE 의 문제점] 

또 다른 문제가 있는데, <u>이미지 내 대부분의 그리드 셀에는 객체가 없습니다</u>. 배경 영역이 전경 영역보다 더 크기 때문입니다. 그리드 셀에 객체가 없다면 confidence score=0입니다. 따라서 <u>대부분의 그리드 셀의 confidence socre=0이 되도록 학습할 수밖에 없습니다. 이는 모델의 불균형을 초래</u>합니다.

이를 개선하기 위해 객체가 존재하는 bounding box 좌표(coordinate)에 대한 loss의 가중치를 증가시키고, 객체가 존재하지 않는 bounding box의 confidence loss에 대한 가중치는 감소시켰습니다. 이는 localization loss와 classification loss 중 localization loss의 가중치를 증가시키고, 객체가 없는 그리드 셀의 confidence loss보다 객체가 존재하는 그리드 셀의 confidence loss의 가중치를 증가시킨다는 뜻입니다. 이로써 위 두 문제가 해결됩니다. 이를 위해 두 개의 파라미터를 사용했는데, **λ_coord**와 **λ_noobj**입니다. λ_coord=5, λ_noobj=0.5로 가중치를 줬습니다.

SSE는 또 다른 문제를 가지고 있습니다. SSE는 큰 bounding box와 작은 boudning box에 대해 <u>모두 동일한 가중치로 loss를 계산</u>합니다. 하지만 작은 bounding box가 큰 bounding box보다 작은 위치 변화에 더 민감합니다. 큰 객체를 둘러싸는 bounding box는 조금 움직여도 여전히 큰 객체를 잘 감싸지만, 작은 객체를 둘러싸는 bounding box는 조금만 움직여도 작은 객체를 벗어나게 됩니다. 이를 개선하기 위해 bounding box의 너비(widht)와 높이(hegith)에 square root를 취해주었습니다. <u>너비와 높이에 square root를 취해주면 너비와 높이가 커짐에 따라 그 증가율이 감소해 loss에 대한 가중치를 감소시키는 효과</u>가 있기 때문입니다.

YOLO는 하나의 그리드 셀 당 여러 개의 bounding box를 예측합니다. 훈련(training) 단계에서 하나의 bounding box predictor가 하나의 객체에 대한 책임이 있어야 합니다. 즉, 객체 하나당 하나의 bounding box와 매칭을 시켜야 합니다. 따라서 여러 개의 bounding box 중 하나만 선택해야 합니다. 이를 위해 예측된 여러 bounding box 중 실제 객체를 감싸는 ground-truth boudning box와의 IOU가 가장 큰 것을 선택합니다. ground-truth boudning box와의 IOU가 가장 크다는 것은 객체를 가장 잘 감싼다는 뜻과 같습니다. 이렇게 훈련된 bounding box predictor는 특정 크기(size), 종횡비(aspect ratios), 객체의 클래스(classes of object)를 잘 예측하게 됩니다.

훈련 단계에서 사용하는 loss function은 다음과 같습니다.

![img](https://blog.kakaocdn.net/dn/cDocKb/btqKwSxuIQK/AO4jemksYpCGktMP2pFoWK/img.png)

좀 복잡하지만 하나하나 살펴보면 그리 어렵지 않습니다. 여기서 1_i^obj는 그리드 셀 i 안에 객체가 존재하는지 여부를 의미합니다. 이 값은 객체 존재하면 1, 존재하지 않으면 0입니다. 1_ij^obj는 그리드 셀 i의 j번째 bounding box predictor가 사용되는지 여부를 의미합니다. 위 loss function의 5개 식은 차례대로 아래와 같은 의미를 갖습니다.

(1) Object가 존재하는 그리드 셀 i의 bounding box predictor j에 대해, x와 y의 loss를 계산.
(2) Object가 존재하는 그리드 셀 i의 bounding box predictor j에 대해, w와 h의 loss를 계산. 큰 box에 대해서는 작은 분산(small deviation)을 반영하기 위해 제곱근을 취한 후, sum-squared error를 구합니다. (같은 error라도 큰 box의 경우 상대적으로 IOU에 영향을 적게 줍니다.)
(3) Object가 존재하는 그리드 셀 i의 bounding box predictor j에 대해, confidence score의 loss를 계산. (Ci = 1)
(4) Object가 존재하지 않는 그리드 셀 i의 bounding box predictor j에 대해, confidence score의 loss를 계산. (Ci = 0)
(5) Object가 존재하는 그리드 셀 i에 대해, conditional class probability의 loss를 계산. (p_i(c)=1 if class c is correct, otherwise: p_i(c)=0)

λ_coord: coordinates(x, y, w, h)에 대한 loss와 다른 loss들과의 균형을 위한 balancing parameter.
λ_noobj: 객체가 있는 box와 없는 box 간에 균형을 위한 balancing parameter. (일반적으로 image내에는 객체가 있는 그리드 셀보다는 없는 셀이 훨씬 많으므로)

출처: [curt-park.github.io/2017-03-26/yolo/](https://curt-park.github.io/2017-03-26/yolo/)

YOLO 연구진은 파스칼 VOC 2007, 2012 훈련 및 검증 데이터 셋을 활용하여 135 epochs로 YOLO 모델을 훈련을 시켰습니다. 이때 batch size=64, momentum=0.9, decay=0.0005로 설정했습니다. 초반에는 학습률(learning rate)을 0.001에서 0.01로 천천히 상승시켰습니다. 만일 처음부터 높은 learning rate로 훈련시켰다면 기울기 폭발(gradient explosion)이 발생하기 때문에 처음에는 작은 값부터 시작한 것입니다. 이후 75 epoch 동안에는 0.01, 30 epoch 동안에는 0.001, 그리고 마지막 30 epoch 동안은 0.0001로 learning rate를 설정했습니다. learning rate를 처음에는 점점 증가시켰다가 다시 감소시켰습니다.

과적합(overfitting)을 막기 위해 드롭아웃(dropout)과 data augmentation을 적용했습니다. 드롭아웃 비율은 0.5로 설정했습니다. data augmentation을 위해 원본 이미지의 20%까지 랜덤 스케일링(random scaling)과 랜덤 이동(random translation)을 적용했습니다. 

[요약]

**2. 2. Training 요약:**
 \- ImageNet 데이터 셋으로 YOLO의 앞단 <u>20개의 컨볼루션 계층을 사전 훈련</u>시킨다.
 \- 사전 훈련된 20개의 컨볼루션 계층 뒤에 <u>4개의 컨볼루션 계층 및 2개의 전결합 계층을 추가</u>한다.
 \- YOLO 신경망의 마지막 계층에는 **선형 활성화 함수(linear activation function)**를 적용하고, <u>나머지 모든 계층에는 **leaky ReLU**를 적용</u>한다.
 \- 구조상 문제 해결을 위해 아래 3가지 개선안을 적용한다.

1) localization loss와 classification loss 중 localization loss의 가중치를 증가시킨다.
2) 객체가 없는 그리드 셀의 confidence loss보다 **객체가 존재하는 그리드 셀의 confidence loss의 가중치를 증가**시킨다.
3) bounding box의 너비(widht)와 높이(hegith)에 square root를 취해준 값을 <u>loss function으로 사용</u>한다.

 \- 과적합(overfitting)을 막기 위해 **드롭아웃(dropout)**과 **data augmentation**을 적용한다.

# 2. 3. Inference

훈련 단계와 마찬가지로, 추론 단계에서도 테스트 이미지로부터 객체를 검출하는 데에는 하나의 신경망 계산만 하면 됩니다. 파스칼 VOC 데이터 셋에 대해서 YOLO는 한 이미지 당 98개의 bounding box를 예측해주고, 그 bounding box마다 클래스 확률(class probabilities)을 구해줍니다. YOLO는 테스트 단계에서 굉장히 빠릅니다. 왜냐하면 YOLO는 R-CNN 등과 다르게 하나의 신경망 계산(a single network evaluation)만 필요하기 때문입니다. 

하지만 YOLO의 그리드 디자인(grid design)은 한 가지 단점이 있습니다. 하나의 객체를 여러 그리드 셀이 동시에 검출하는 경우가 있다는 점입니다. 객체의 크기가 크거나 객체가 그리드 셀 경계에 인접해 있는 경우, 그 객체에 대한 bounding box가 여러 개 생길 수 있습니다. 즉, 하나의 그리드 셀이 아닌 여러 그리드 셀에서 해당 객체에 대한 bounding box를 예측할 수 있다는 뜻입니다. 하나의 객체가 정확히 하나의 그리드 셀에만 존재하는 경우에는 이런 문제가 없지만 객체의 크기, 객체의 위치에 따라 충분히 이런 문제가 발생할 수 있습니다. 이를 다중 검출(multiple detections) 문제라고 합니다. 이런 다중 검출(multiple detections) 문제는 비 최대 억제(non-maximal suppression)라는 방법을 통해 개선할 수 있습니다. YOLO는 비 최대 억제를 통해 mAP를 2~3%가량 향상시켰습니다.

# 2. 4. Limitations of YOLO

YOLO는 하나의 그리드 셀마다 두 개의 bounding box를 예측합니다. 그리고 하나의 그리드 셀마다 오직 하나의 객체만 검출할 수 있습니다. 이는 공간적 제약(spatial constraints)을 야기합니다. 공간적 제약이란 '하나의 그리드 셀은 오직 하나의 객체만 검출하므로 하나의 그리드 셀에 두 개 이상의 객체가 붙어있다면 이를 잘 검출하지 못하는 문제'를 뜻합니다. 예를 들어, 새 떼와 같이 작은 물체가 몰려 있는 경우 공간적 제약 때문에 객체 검출이 제한적일 수밖에 없습니다. 하나의 그리드 셀은 오직 하나의 객체만 검출하는데 여러 객체가 몰려있으면 검출하지 못하는 객체도 존재하는 것이죠.

그리고 YOLO 모델은 데이터로부터 bounding box를 예측하는 것을 학습하기 때문에 훈련 단계에서 학습하지 못했던 새로운 종횡비(aspect ratio, 가로 세로 비율)를 마주하면 고전할 수밖에 없습니다. 

마지막으로 YOLO 모델은 큰 bounding box와 작은 bounding box의 loss에 대해 동일한 가중치를 둔다는 단점이 있습니다. 크기가 큰 bounding box는 위치가 약간 달라져도 비교적 성능에 별 영향을 주지 않는데, 크기가 작은 bounding box는 위치가 조금만 달라져도 성능에 큰 영향을 줄 수 있습니다. 큰 bounding box에 비해 작은 bounding box가 위치 변화에 따른 IOU 변화가 더 심하기 때문입니다. 이를 부정확한 localization 문제라고 부릅니다.

[요약]

 \- 작은 객체들이 몰려있는 경우 검출을 잘 못한다.
 \- 훈련 단계에서 학습하지 못한 종횡비(aspect ratio)를 테스트 단계에서 마주치면 고전한다.
 \- 큰 bounding box와 작은 bounding box의 loss에 대해 동일한 가중치를 둔다.

# 3. Comparison to Other Detection Systems

**Deformable parts models(DPM)**

객체 검출 모델 중 하나인 DPM은 **슬라이딩 윈도(sliding window)** 방식을 사용합니다. DPM은 하나로 연결된 파이프라인이 아니라 <u>서로 분리된 **파이프라인**으로 구성</u>되어 있습니다. 독립적인 파이프라인이 각각 **특징 추출(feature extraction)**, **위치 파악(region classification)**, **bounding box 예측(bounding box prediction)** 등을 수행합니다. 반면 YOLO는 이렇게 분리된 파이프라인을 하나의 컨볼루션 신경망으로 대체한 모델입니다. 이 신경망은 특징 추출, bounding box 예측, 비 최대 억제 등을 한 번에 처리합니다. 따라서 <u>YOLO는 DPM보다 더 빠르고 정확</u>합니다.

> DPM의 방식
>
> 1. **초기 검출 창 생성**: 이미지에서 <u>다양한 크기와 비율의 초기 검출 창을 생성</u>합니다. 이 초기 창들은 후속 단계에서 <u>객체를 검출하기 위한 후보 영역으로 사용</u>됩니다.
> 2. **이미지 피라미드 생성**: 초기 검출 창에 다양한 스케일의 **이미지 피라미드**를 생성합니다. 이미지 피라미드는 객체가 다양한 크기로 나타날 수 있는 경우에 대비하여 검출 성능을 향상시키는 데 도움이 됩니다.
> 3. **객체 모델 초기화**: DPM은 객체 모델을 사용하여 객체를 검출합니다. 객체 모델은 특징 디스크립터와 그에 대응하는 특징 파트들로 구성됩니다. 객체 모델의 초기 상태를 설정합니다.
> 4. **객체 모델 학습**: 학습 데이터셋에서 객체 모델을 학습시킵니다. 학습은 객체의 특징과 배경의 특징을 구분할 수 있는 분류기를 학습하는 과정입니다. 객체의 특징과 배경의 특징을 구분하기 위해 SVM(Support Vector Machine)과 같은 분류 알고리즘이 사용될 수 있습니다.
> 5. **객체 검출**: 이미지 피라미드와 초기 검출 창을 사용하여 객체를 검출합니다. 검출 창과 객체 모델을 비교하여 객체의 위치와 확률을 예측합니다. 일정한 임계값을 기준으로 객체를 판별합니다.
> 6. **후처리**: 겹치는 검출 창을 제거하거나 객체들을 병합하는 등의 후처리 단계를 수행하여 최종 객체 검출 결과를 정제합니다.

**R-CNN**

R-CNN은 슬라이딩 윈도 대신 region proposal 방식을 사용하여 객체를 검출하는 모델입니다. selective search라는 방식으로 여러 bounding box를 생성하고, 컨볼루션 신경망으로 feature를 추출하고, SVM으로 bounding box에 대한 점수를 측정합니다. 그리고 선형 모델(linear model)로 bounding box를 조정하고, 비 최대 억제(non-max suppression)로 중복된 검출을 제거합니다. 이 복잡한 파이프라인을 각 단계별로 독립적으로 튜닝해야 하기 때문에 R-CNN은 속도가 굉장히 느립니다. 테스트 단계에서 한 이미지를 처리하는데 40초 이상이 걸립니다. 정확성은 높지만 속도가 너무 느려 실시간 객체 검출 모델로 사용하기에는 한계가 있습니다.

YOLO는 R-CNN와 비슷한 면도 있습니다. 각 그리드 셀이 bounding box를 예측한다는 것, 그 box에 대해 점수를 계산한다는 것이 비슷합니다. 하지만 YOLO는 각 그리드 셀의 공간적 제약 때문에 하나의 객체가 여러 번 검출되는 경우가 R-CNN에 비해 적습니다. 그리고 YOLO는 R-CNN에 비해 예측하는 bounding box의 개수도 훨씬 적습니다. 한 이미지 당 bounding box가 2,000개인 R-CNN에 비해 YOLO는 98개로 훨씬 적습니다. 그리고 다시 말하지만 YOLO는 이 모든 절차를 단일 모델로 수행합니다.

# 4. Experiments

먼저, YOLO를 다른 실시간(real-time) 객체 검출 모델과 비교해보겠습니다. YOLO와 Fast R-CNN의 성능의 차이를 비교하기 위해 파스칼 VOC 2007 데이터 셋에서의 에러를 구해봤습니다. Fast R-CNN은 이 논문이 나온 시점을 기준으로 성능이 가장 좋은 R-CNN 계열의 모델이었습니다. 

# 4. 1. Comparison to Other Real-Time Systems

객체 검출에 대한 많은 연구들은 표준화된 객체 검출 파이프라인을 빠르게 만드는데 초점을 두고 있습니다. YOLO 연구진은 GPU 버전의 YOLO와 30Hz/100Hz에서의 DPM의 성능을 비교해보았습니다.

Fast YOLO는 파스칼 데이터 셋 기준으로 가장 빠른 객체 검출 모델입니다. mAP는 52.7%입니다. 이는 30Hz/100Hz에서의 DPM보다 2배 높은 정확도입니다. YOLO는 실시간(real-time) 성능은 그대로 유지하며 mAP를 63.4%까지 높인 모델입니다.

또한 YOLO 연구진은 VGG-16을 사용하여 YOLO를 훈련시켰습니다. 이 모델은 일반 YOLO에 비해 mAP가 더 높지만 속도는 다소 느렸습니다. VGG-16을 사용하는 다른 객체 검출 모델과 비교하는 데에는 유용하지만 실시간 객체 검출 모델로 사용하기에는 속도가 느려, 이 논문에서는 속도가 빠른 YOLO에 집중해 설명했습니다.

Fast DPM은 DPM의 mAP를 약간 하락시키면서 속도를 향상시킨 모델입니다. 그러나 실시간(real-time) 검출에 사용하기에는 여전히 속도가 느립니다. 그리고 신경망을 이용한 객체 검출 모델에 비해 정확도도 떨어지는 편입니다.

R-CNN minus R 모델은 selective search를 정적 bounding box proposal로 대체한 모델입니다. R-CNN에 비해서는 훨씬 빠르지만 실시간(real-time) 검출에는 여전히 부족합니다. Fast R-CNN은 높은 정확도를 보이고 R-CNN보다 빠르지만 초당 0.5 프레임만을 처리할 수 있어 실시간 검출에 사용하기엔 역부족입니다.

Faster R-CNN 모델은 bounding box proposal을 위해 selective search를 사용하지 않고 신경망을 사용합니다. 그리하여 기존의 R-CNN 계열보다는 속도가 빨라졌지만 여전히 YOLO에 비해서는 몇 배나 느립니다.

아래 표는 각종 객체 검출 모델 별 정확도(mAP)와 속도(FPS)를 보여줍니다. FPS가 30 이상은 되어야 실시간 검출로 사용할 수 있는 것 같습니다. FPS가 30이라는 것은 1초에 30 프레임의 영상을 처리한다는 뜻입니다. 정확도는 Fast R-CNN과 Faster R-CNN VGG-16이 가장 높지만 이 모델들의 FPS는 너무 낮아 실시간 객체 검출 모델로 사용할 수는 없습니다. 반면, 정확도도 적당히 높고 속도도 빠른 모델은 YOLO 계열인 것을 알 수 있습니다.

![img](https://blog.kakaocdn.net/dn/cjWST3/btqKxP8s1CP/KWlVFVkwSuRW9eikFZwpt1/img.png)

[요약]

**4. 1. Comparison to Other Real-Time Systems 요약:**
 \- YOLO는 기존 모델에 비해 속도는 월등히 빠르고, 정확도도 꽤 높은 수준이다.

# 4. 2. VOC 2007 Error Analysis

YOLO와 기존 객체 검출 모델을 더 비교해보겠습니다. 우선, 파스칼 VOC 2007 데이터 셋에 대해 YOLO와 Fast R-CNN의 성능을 비교해보았습니다. 이를 위해 [Diagnosing Error in Object Detectors](http://dhoiem.web.engr.illinois.edu/publications/eccv2012_detanalysis_derek.pdf) 논문에 소개된 에러 측정 방법론을 사용했습니다. 다음과 같은 기준으로 객체 검출이 정확한지, 틀렸다면 어떤 error type인지를 구분했습니다.

Correct : class가 정확하며 IOU > 0.5 인 경우
Localization : class가 정확하고, 0.1 < IOU < 0.5 인 경우
Similar : class가 유사하고 IOU > 0.1 인 경우
Other : class는 틀렸으나, IOU > 0.1 인 경우
Background : 어떤 Object라도 IOU < 0.1 인 경우

Fast R-CNN과 YOLO를 비교한 결과는 다음과 같습니다.

![img](https://blog.kakaocdn.net/dn/cPr0TS/btqKwrUOhKG/f2U9KwbHuDaUa8TWMaAac1/img.png)

YOLO는 localization error 가 상대적으로 큽니다. localization error는 19.0%로 나머지 error를 모두 합한 15.5%(6.75%+4.0%+4.75%) 보다 큽니다. Fast R-CNN은 YOLO에 비해 localization error가 작습니다. 반면, background error가 상대적으로 큽니다. backgound error는 배경에 아무 물체가 없는데 물체가 있다고 판단하는 false positive error입니다. Fast R-CNN은 YOLO에 비해 background error가 3배나 더 큽니다.

# 4. 3. Combining Fast R-CNN and YOLO

YOLO는 Fast R-CNN에 비해 background error가 훨씬 적습니다. 따라서 Fast R-CNN에 YOLO를 결합하여 background error를 줄인다면 굉장히 높은 성능을 낼 수 있을 것입니다. R-CNN이 예측하는 모든 boudning box에 대해 YOLO도 유사하게 예측하는지를 체크하면 됩니다. 만약 R-CNN이 예측한 bounding box와 YOLO가 예측한 bounding box가 유사하다면 두 bounding box가 겹치는 부분을 bounding box로 잡으면 됩니다. 

파스칼 VOC 2007 데이터 셋에 대해 가장 성능이 좋은 Fast R-CNN 모델은 71.8%의 mAP를 기록했습니다. Fast R-CNN과 YOLO를 결합하면 mAP가 3.2% 올라 75.0%가 됩니다. Fast R-CNN과 다른 모델과도 앙상블을 해봤지만 mAP 향상은 0.3%, 0.6%로 미미했습니다. 

물론 Fast R-CNN과 YOLO를 결합한 모델은 YOLO에 비해 느립니다. 왜냐하면 Fast R-CNN과 YOLO를 독립적으로 돌려 결과를 앙상블 하는 방식이기 때문입니다. 그렇지만 YOLO가 워낙 빨라 Fast R-CNN을 단독으로 돌리는 것과 앙상블 모델을 돌리는 것의 속도는 거의 유사합니다. 그 말은 Fast R-CNN을 사용하는 것보다는 Fast R-CNN과 YOLO를 결합한 모델을 사용하는 것이 더 낫다는 뜻입니다.

![img](https://blog.kakaocdn.net/dn/bmcu3F/btqKC9SqmeH/0YhihLRyD9UP48WdlPuH51/img.png)

# 4. 4. VOC 2012 Results

파스칼 VOC 2012 데이터 셋에서 YOLO는 57.9%의 mAP를 달성했습니다. 이는 VGG-16을 사용한 R-CNN의 mAP와 비슷합니다. 아래 표를 참고하시기 바랍니다.

![img](https://blog.kakaocdn.net/dn/0yLEZ/btqKxlfBqLX/7caDpYNgteLXb6a2l3EKW1/img.png)

속도 측면에서는 YOLO가 빠르고, 정확도 측면에서는 Fast R-CNN과 YOLO를 결합한 모델이 가장 좋습니다.

# 4. 5. Generalizability: Person Detection in Artwork

객체 검출 연구를 위해 사용하는 데이터 셋은 훈련 데이터 셋과 테스트 데이터 셋이 동일한 분포를 지닙니다. 하지만 실제 이미지 데이터는 훈련 데이터 셋과 테스트 데이터 셋의 분포가 다를 수 있습니다. YOLO 연구진은 훈련 데이터 셋과 다른 분포는 지닌 테스트 데이터 셋(즉, 훈련 데이터 셋에서 보지 못한 새로운 데이터 셋)을 활용하여 테스트해봤습니다. 여기서는 피카소 데이터 셋과 일반 예술 작품을 사용했습니다. 훈련 단계에서는 실제 이미지로 학습했지만 테스트 단계에서는 예술 작품을 활용해 테스트해보는 것입니다.

아래 표는 YOLO과 다른 객체 검출 모델의 성능을 측정한 것입니다. 파스칼 VOC 2007에서 학습한 YOLO, R-CNN, DPM 등의 성능을 서로 비교했습니다. R-CNN은 VOC 2007에서는 높은 정확도를 보이지만 예술작품에 대해서는 굉장히 낮은 정확도를 보입니다. DPM은 예술 작품에 대해서도 정확도가 크게 떨어지지는 않았습니다. 다만 VOC 2007에서의 정확도도 그리 높은 편은 아닙니다. 반면 YOLO는 VOC 2007에서도 가장 높은 정확도를 보였고, 예술 작품에 대해서도 정확도가 크게 떨어지지 않았습니다.

![img](https://blog.kakaocdn.net/dn/Q1WRs/btqKEAvAEaG/DiERiGPyzUewrLB5XlCLY0/img.png)

[요약]

**4. 5. Generalizability: Person Detection in Artwork 요약:**
 \- YOLO는 훈련 단계에서 접하지 못한 새로운 이미지도 잘 검출한다.

# 5. Real-Time Detection In The Wild

YOLO는 컴퓨터 비전 애플리케이션에 활용할 수 있는 빠르고 정확한 객체 검출 모델입니다. 연구진은 YOLO를 웹캠과 연결하여 실시간(real-time) 객체 검출을 수행했습니다. 이는 [웹사이트](http://pjreddie.com/yolo/)를 통해 확인할 수 있습니다.

![img](https://blog.kakaocdn.net/dn/b4jNek/btqKC9dWBG3/JooV4uayytdVJFNrs6wXfk/img.png)

# 6. Conclusion

지금까지 객체 검출을 위한 통합 모델인 YOLO에 대해 소개했습니다. YOLO는 <u>단순하면서도 빠르고 정확</u>합니다. 또한 YOLO는 훈련 단계에서 보지 못한 <u>새로운 이미지에 대해서도 객체를 잘 검출</u>합니다. 즉, 새로운 이미지에 대해서도 강건하여 <u>애플리케이션에서도 충분히 활용할만한 가치</u>가 있습니다.

# 7. 의의



# 7.1 어떻게 경량화 시킬까?

### 경량화 방법

1. 모델 아키텍처 변경: YOLO 모델은 기본적으로 딥러닝 신경망이므로, 모델의 아키텍처를 변경하여 경량화할 수 있습니다. 예를 들어, 더 작은 컨볼루션 필터, 레이어 수의 감소, 다운샘플링 스텝 수의 감소 등을 고려할 수 있습니다.
2. 네트워크 압축: 모델의 크기를 줄이기 위해 다양한 압축 기법을 사용할 수 있습니다. 예를 들어, 가중치 정규화, 가중치 클러스터링, 양자화, 가중치 절단 등의 기법을 사용하여 모델의 파라미터 수를 줄일 수 있습니다.
3. 프루닝(Pruning): 프루닝은 불필요한 가중치를 제거하여 모델을 경량화하는 기술입니다. 작은 가중치 값이나 중요하지 않은 연결을 제거함으로써 모델의 크기를 줄일 수 있습니다.
4. 층 병합(Layer Fusion): 여러 레이어를 하나로 병합하여 중복되는 계산을 줄이고 모델의 크기를 줄일 수 있습니다. 예를 들어, 합성곱 레이어와 배치 정규화 레이어를 하나의 레이어로 병합하는 등의 기법을 사용할 수 있습니다.
5. 경량화된 백본(Backbone): YOLO 모델의 백본은 객체 감지에 중요한 역할을 합니다. 경량화된 백본 아키텍처를 사용하여 모델의 크기를 줄일 수 있습니다. 예를 들어, MobileNet, ShuffleNet, EfficientNet 등 경량화된 백본 아키텍처를 고려해 볼 수 있습니다.

### YOLO의 특징

YOLO는 최신(SOTA, state-of-the-art) 객체 검출 모델에 비해 <u>정확도가 다소 떨어진다는 단점</u>이 있습니다. 빠르게 객체를 검출할 수 있다는 장점은 있지만 정확성이 다소 떨어집니다. 특히 <u>작은 물체에 대한 검출 정확도가 떨어집니다</u>



# 7.2 왜 돌아가는지?



# 7.3 왜 중요한지?



# 출처

- [You Only Look Once: Unified, Real-Time Object Detection] https://arxiv.org/pdf/1506.02640.pdf
- [논문 리뷰 - YOLO(You Only Look Once) 톺아보기] https://bkshin.tistory.com/entry/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-YOLOYou-Only-Look-Once
- [[PR12] You Only Look Once (YOLO): Unified Real-Time Object Detection] https://www.slideshare.net/TaegyunJeon1/pr12-you-only-look-once-yolo-unified-realtime-object-detection
- [YOLO CVPR 2016] https://docs.google.com/presentation/d/1kAa7NOamBt4calBU9iHgT8a86RRHz9Yz2oh4-GTdX6M/edit#slide=id.p
- [Hello Blog - YOLO 논문 리뷰] https://curt-park.github.io/2017-03-26/yolo/
- [모두를 위한 객체 검출 - You Only Look Once - Paper Review] https://deepbaksuvision.github.io/Modu_ObjectDetection/posts/04_01_Review_of_YOLO_Paper.html
- [OpenCV Practice 14\] 이미지 피라미드 (Image Pyramid)] https://dsbook.tistory.com/219
- [합성곱 신경망 CNN(기초)] https://wiserloner.tistory.com/480
- [YOLO: You only look once (How it works)] https://www.youtube.com/watch?v=L0tzmv--CGY
- [[분석] YOLO] https://curt-park.github.io/2017-03-26/yolo/