---
layout: posts
title: "사이킷런(scikit-learn)으로 학습한 모델 저장하기"
categories: ML
tag: [ML, Study]
toc: true
---

> 출처: https://gaussian37.github.io/ml-sklearn-saving-model/

***

sklearn을 이용하여 model을 학습한 후 학습한 결과를 저장하는 방법에 대하여 알아보겠습니다.

pickle 형태로 모델을 저장할 것이고 저장할 때에는 sklearn의 joblib을 사용할 것입니다. pickle은 파이썬에서 지원하는 serializer 형태의 저장 방식입니다. 참고로 JSON 같은 경우는 언어에 상관없이 범용적으로 사용할 수 있는 seriazlier 형태이지만 pickle은 파이썬에서만 사용가능 하되 지원되는 데이터 타입이 JSON 보다 많이 있습니다.

# iris 데이터를 사용해 pickle 형태로 모델 저장

```
from sklearn.linear_model import LogisticRegression
from sklearn import datasets
import pickle
from sklearn.externals import joblib
```

# 데이터 로드
```
# Load the iris data
iris = datasets.load_iris()

# Create a matrix, X, of features and a vector, y.
X, y = iris.data, iris.target
```

# logistic regression 적용
```
clf = LogisticRegression(random_state=0)
clf.fit(X, y)
```

# 모델 변수에 저장
```
saved_model = pickle.dumps(clf)
```

* saved_model 을 실행해 보면 이상한 문자열이 나오는데 그것이 serializer 형태로 저장된 것이라고 볼 수 있습니다.

# pickle로 저장한 모델 불러오기
```
# Load the pickled model
clf_from_pickle = pickle.loads(saved_model)

# Use the loaded pickled model to make predictions
clf_from_pickle.predict(X)
>> array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1,
       1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2,
       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])
```

# 파일에 저장
```
joblib.dump(clf, 'filename.pkl')

>>

['filename.pkl',
 'filename.pkl_01.npy',
 'filename.pkl_02.npy',
 'filename.pkl_03.npy',
 'filename.pkl_04.npy']
```

# 저장된 파일을 불러와 predict 진행
```
clf_from_joblib = joblib.load('filename.pkl')
clf_from_joblib.predict(X)

>>> array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1,
       1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2,
       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])
```