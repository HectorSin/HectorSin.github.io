---
layout: posts
title: "ML 기초수학 Ch01"
categories: ML
tag: [ML, Study]
toc: true
---

## 1. 경사도벡터(Gradient Vector)

---

가중치(weight) 업데이트에 사용되는 경사하강법에 대해 정리하며 변수가 벡터인 경우의 기울기를 나타내는 gradient vector를 다룰 것이다.

### **1. 미분(differentiation)**

---

- 목적함수 최대화 (**경사상승법**)

  > 함숫값을 높이기 위해서는 미분값이 양수이든 음수이든 현재의 위치에서 미분값을 더하면 된다. 이는 목적함수를 최대화할 때 사용한다.

- 목적함수 최소화 (**경사하강법**)
  > 함숫값을 줄이기 위해서는 미분값이 양수이든 음수이든 현재의 위치에서 미분값을 빼주면 된다. 이는 목적함수를 최소화할 때 사용한다.

### **2. 경사하강법**

---

Loss함수를 최소화하기 위해서는 경사하강법을 이용하여 weight를 갱신할 수 있다. 경사하강법은 weight의 기울기를 이용하여 함숫값이 낮아지는 방향으로 weight를 이동시켜 극값에 도달할 때까지 반복하는 것이다. 경사하강법을 통해 도달한 극값은 항상 최솟값을 보장하지는 않는다.

미분은 함수 f의 주어진 점 (x, f(x))에서의 접선의 기울기이다.

### **3. Gradient Vector**

---

- 편미분(partial differenciation)

  > 변수가 벡터인 다변량 함수의 경우 편미분(partial differentiation)을 사용하여 기울기를 계산한다.

- Gradient Vector
  > 변수 d개에 대해 각각 편미분 한 것을 한 번에 표시한다.

![gradient vector](/images/2023-01-05-ML_1/gradient vector.png)

> 저 역삼각형 기호는 nabla라고 하며 gradient vector임을 나타내는 기호이다. f'(x) 대신 nabla를 사용하여 변수 x=(x1, x2, ···, xd)를 **동시에 update**할 수 있다.

### **4. 확률적 경사하강법(Stochastic Gradient Descent, SGD)**

---

> 경사하강법은 미분가능하고 볼록(convex)한 함수에 대해서는 학습률과 학습횟수가 적절하다면 최소점으로 수렴한다는 것이 보장되어있다. 선형함수도 weight 즉 beta에 대해서 볼록함수를 가지기 때문에 최소점을 잘 찾아간다. 하지만 비선형함수나 목적식이 볼록하지 않은(non-convex) 경우 수렴이 보장되지 않는다.

![확률적_경사하강법](/images/2023-01-05-ML_1/확률적_경사하강법.png)

> 이를 보완하기 위한 방법이 확률적 경사하강법이다. 확률적 경사하강법은 모든데이터를 한 번에 업데이트하지 않고 한개 또는 일부를 활용하여 업데이트하는 것을 말한다. 이때 데이터 일부를 미니배치라 한다.

- 이점
  > 첫번째는 데이터의 일부를 사용하기 때문에 연산 자원 측면에서 효율적이다. 하드웨어의 제한을 극복하고 병렬 계산이 가능하도록 하기 때문이다.
  > 두번째는 매번 다른 미니 배치를 사용하므로 목적식이 다르다. 따라서 매번 다른 곡선을 가지게 되는데 이것은 gradient가 한 번 0에 도달하더라도 다른 미니배치가 다른 곡선을 통해 계속적으로 update할 수 있기 때문에 local minimum을 지날 수 있다.

## 2. η(에타)

---

### 그리스 문자 발음과 의미

---

- Η η →에타(ETA) : 효율을 나타내는 기호로 쓴다.

## 3. 지도학습 / 비지도학습 / 강화학습

## 4. 로지스틱 회귀

성현이 보고 있나

참고: https://amber-chaeeunk.tistory.com/71 [[경사하강법] 미분 , 경사하강법 , gradient vector , 확률적 경사하강법 ( SGD )]
