---
layout: posts
title: "딥러닝 모델 압축 방법론과 BERT 압축"
categories: DL
tag: [coding, DL, Concept]
toc: true
---

> [참고] - https://blog.est.ai/2020/03/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EB%AA%A8%EB%8D%B8-%EC%95%95%EC%B6%95-%EB%B0%A9%EB%B2%95%EB%A1%A0%EA%B3%BC-bert-%EC%95%95%EC%B6%95/

# 배경
***

딥러닝(Deep Learning)은 뛰어난 성능과 **높은 모델의 확장성(Scalability)**으로 인해 많은 주목을 받았고, 요즘 산업계에서도 활발하게 적용되고 있습니다. 하지만 모델의 높은 확장성은 또 다른 문제를 불러오게 되었습니다.

## 단점
***

기본적으로 딥러닝 모델의 성능은 그 크기에 비례하는 경향을 보입니다. 그렇다면 우리가 좋은 성능의 모델을 얻기 위해서는 계속해서 모델을 키우기만 하면 될까요? 딥러닝 모델이 커지면 어떤 문제점들이 있을까요? 작은 딥러닝 모델로도 큰 모델과 같은 성능을 얻을 수 있을까요?

이 글에서는 이러한 의문점을 연구하는 분야인 **모델 압축(Model Compression)**에 대해 이야기 해보려고 합니다. 특히, 현재 자연어처리(Natural Language Processing, NLP) 분야에서의 모델 발전 방향과 문제점, 모델 압축 방법론의 등장, 관련된 논문들을 다루려고 합니다.

# Big Learning: Larger Dataset, Larger Model, Pre-training, Fine-tuning
***

## 딥러닝 배경
***

딥러닝은 CNN(Convolutional Neural Network)이 2012년 ILSVRC(ImageNet Large-Scale Visual Recognition Challenge)에서 뛰어난 성능으로 우승을 하며, 큰 주목을 받았습니다. 그 이후로 컴퓨터 비전 분야에서는 VGGNet(2014), GoogLeNet(2015), ResNet(2015), DenseNet(2016) 등의 다양한 네트워크 아키텍처가 등장하여 모델을 점점 깊고 크게 만들어 나갔고, 이러한 거대한 모델을 거대한 이미지 데이터셋을 이용하여 미리 학습(Pre-train)시키고, 이를 특정 응용 분야에 맞춰 새로 학습(Fine-tune)하는 방식의 접근법이 주류가 되었습니다.

## NLP
***

### NLP의 한계
***

반면, 자연어 처리(NLP) 분야의 초기 딥러닝 연구 방향은 비전 분야와는 약간 달랐습니다. 자연어 처리에서는 'RNN(Recurrent Neural Network)'이라고 하는 연속된 데이터를 다루는데 특화된 모델을 주로 사용하였습니다. RNN은 모델이 처리해야 하는 **데이터가 길어짐에 따라 기울기(Gradient)가 사라진다는 문제**와 다음 입력 데이터 처리를 위해 이전 데이터가 필요하여 **병렬화가 어렵다는 문제**가 있었습니다. RNN 모델이 가진 한계로 인해, 자연어 처리에서는 컴퓨터 비전에서만큼 모델을 거대화하기 어려웠습니다.

또한, 컴퓨터 비전에서는 ImageNet에서 분류 학습을 한 모델이 다른 분야에서도 필요한 특징을 잘 뽑아내었는데요. 자연어 처리에서는 이에 대응되는 거대한 데이터셋과 다른 분야에서 특징을 잘 뽑아내기 위한 사전 학습 방법이 잘 알려져 있지 않아, **이미 학습된 모델이 다른 분야에 활용되기 어려웠습니다**. 자연어 처리 분야에서 사전 학습을 위한 주된 접근법은 거대한 말뭉치(Corpus)에서 단어 임베딩(Word embedding)을 학습하여 재사용하는 정도였습니다.

### 해결법
***

하지만 2018년에 Google이 BERT(Bidirectional Encoder Representations from Transformers)를 발표하며, 자연어 처리 분야에서도 거대한 모델들이 속속 등장하기 시작하였습니다. BERT는 Transformer 기반의 모델로, 자연어 처리에서도 컴퓨터 비전과 마찬가지로 거대한 모델의 사전 학습 - 재학습이 가능해졌고, 다양한 문제들에서 뛰어난 성능을 보여주었습니다.

이후 다양한 형태의 바리에이션이 나오면서 BERT는 현재 NLP 연구의 주류가 되었습니다. 현재 NLP 연구는 거대한 모델을 만들고, 많은 데이터를 이용해 모델을 미리 학습한 후 응용 분야에 맞춰 재학습하는 접근 방식을 취하고 있습니다.

![BERT_model](/images/2022-25-Bert/BERT_model.png)

## 모델 압축의 필요성
***

딥러닝을 이용해 해결하려는 문제가 복잡해지면서 요구되는 모델의 크기가 급격하게 증가하게 되었고, 이에 따라 다양한 문제점들이 등장하기 시작했습니다.

![nlp_prob](/images/2022-25-Bert/nlp_prob.png)

# 딥러닝 모델 크기가 증가함에 따라 발생 가능한 문제들
***

1. Memory Limitation

2. Training/Inference Speed

3. Worse Performance

4. Practical problems